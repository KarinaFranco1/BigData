# -*- coding: utf-8 -*-
"""ABC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_miAR1stt-s8bh_UaD_9mWuIWiKKrIE8

# Primera Parte
"""

import tweepy
from textblob import TextBlob
from wordcloud import WordCloud, STOPWORDS
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from nltk.stem import SnowballStemmer
import datetime
import string
import nltk

consumer_Key = 'R1QvArr26PTbzhLPSMnHqvBlS'
consumer_secret = 'zt5BpNcPMpMOwPSuBue9arNnRh9SBsbP3jC6MwgVAHc1BQ4ToZ'
access_token = '1389194574162272259-V3ZQWniZ72BGCZWnGrEW39VYxA9shU'
access_token_Secret = '68P8W46ZfhJEnS2IWya4g7378vFCYEHQegxzVZ5dSeoTj'

# OAuth process, using the keys and tokens
auth = tweepy.OAuthHandler(consumer_Key, consumer_secret)
auth.set_access_token(access_token, access_token_Secret)
 
# creation of the actual interface, using authentication
api = tweepy.API(auth)

list_tweets = tweepy.Cursor(api.search, q="#disculpateABCCOLOR",tweet_mode='extended', lang='es').items(300)

output = []
for tweet in list_tweets:
    text = tweet._json["full_text"]
    print(text)
    name = tweet.user.name
    favourite_count = tweet.favorite_count
    retweet_count = tweet.retweet_count
    created_at = tweet.created_at
    
    line = {'text' : text, 'name':name,'favourite_count' : favourite_count, 'retweet_count':retweet_count, 'created_at' : created_at}
    output.append(line)

df = pd.DataFrame(output)
df.to_csv('ABCDigital.csv')

df

"""# Análisis de sentimiento"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

import nltk
nltk.download('vader_lexicon')
nltk.download('punkt')

df=pd.read_csv("ABCDigital.csv")

df['polaridad']=df['text'].apply(lambda x:TextBlob(x).sentiment.polarity)

df

print("Polaridad")
print("Maximo valor ",df['polaridad'].max())
print("Mimimo valor ",df['polaridad'].min())
print("Valor medio ",df['polaridad'].mean())

from nltk.sentiment.vader import SentimentIntensityAnalyzer

df[['polaridad','subjetividad']] = df['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))
for index, row in df['text'].iteritems():
    score = SentimentIntensityAnalyzer().polarity_scores(row)
    neg = score['neg']
    neu = score['neu']
    pos = score['pos']
    comp = score['compound']
    if neg > pos:
        df.loc[index, 'sentiment'] = "negative"
    elif pos > neg:
        df.loc[index, 'sentiment'] = "positive"
    else:
       df.loc[index, 'sentiment'] = "neutral"
    df.loc[index, 'neg'] = neg
    df.loc[index, 'neu'] = neu
    df.loc[index, 'pos'] = pos
    df.loc[index, 'compound'] = comp

df.head(5)

def cleanTxtNew(text):
    text = re.sub(':', '', text) 
    text = re.sub('"', '', text)
    text = re.sub('|', '', text)
    text = re.sub('_', '', text)
    text = re.sub('@[0-9A-Za-z]+', '', text)
    text = re.sub(r'[0-9]+', '', text) 
    text = re.sub(',', '', text)
    text = re.sub('RT[\s]+', '', text)
    text = re.sub('@[A-Za-z0–9]+', '', text)
    text = re.sub('#', '', text)
    text = re.sub('https?:\/\/\S+', '', text)
    text = re.sub('\n', '', text)
    text = re.sub('https\/\/\S+', '', text)
    text = re.sub('UNA', '', text)
    text = re.sub('la', '', text)
    text = re.sub('que', '', text)
    text = re.sub('de', '', text)
    text = re.sub('para', '', text)
    text = re.sub('se', '', text)
    text = re.sub('en', '', text)
    text = re.sub('lo', '', text)
    text = re.sub('el', '', text)
    text = re.sub('un', '', text)
    text = re.sub('como', '', text)
    text = re.sub('si', '', text)
    text = re.sub('pero', '', text)
    return text
df['text'] = df['text'].apply(lambda x: cleanTxtNew(x))

df.head(10)

allWords = ' '.join([twts for twts in df['text']])
wordCloud = WordCloud(width=700, height=500, random_state=30, max_font_size=210).generate(allWords)
plt.imshow(wordCloud, interpolation="bilinear")
plt.axis('off')
plt.show()

df_negative = df[df["sentiment"]=="negative"]
df_positive = df[df["sentiment"]=="positive"]
df_neutral = df[df["sentiment"]=="neutral"]

def count_values_in_column(data,feature):
    total=data.loc[:,feature].value_counts(dropna=False)
    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)
    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])
count_values_in_column(df,"sentiment")

def uno(data,feature):
    total=data.loc[:,feature].value_counts(dropna=False)
    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)
    return total, percentage
aux1,aux2 = uno(df,"sentiment")

plt.title('Sentiment Analysis')
eje_x = ['neutral','positive','negative']
eje_y = [aux2[0],aux2[1],aux2[2]]
plt.barh(eje_x, eje_y)
plt.show()

sns.distplot(df['polaridad'])
sns.distplot(df['subjetividad'])

#Grafico de histograma
x = np.random.normal(df['compound'])
plt.hist(x)
plt.show()